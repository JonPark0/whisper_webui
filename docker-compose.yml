services:
  redis:
    image: redis:7-alpine
    container_name: whisper_redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    networks:
      - whisper_network
    command: redis-server --appendonly yes

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: whisper_backend
    user: "1000:1000"  # Run as non-root user
    ports:
      - "0.0.0.0:${BACKEND_PORT:-8000}:8000"  # Listen on all interfaces
    volumes:
      - ./backend:/app
      - uploads:/app/uploads
      - outputs:/app/outputs
      - models:/app/models
      - data:/app/data
    env_file:
      - .env
    environment:
      - PYTHONUNBUFFERED=1
      - RELOAD_MODE=${RELOAD_MODE:-true}
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - redis
    restart: unless-stopped
    networks:
      - whisper_network

  celery_worker:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: whisper_celery_worker
    user: "1000:1000"  # Run as non-root user
    ipc: host  # Use host IPC for shared memory (required for PyTorch)
    ulimits:
      memlock: -1
      stack: 67108864
    command: celery -A celery_app worker --loglevel=info --pool=solo --concurrency=1
    volumes:
      - ./backend:/app
      - uploads:/app/uploads
      - outputs:/app/outputs
      - models:/app/models
      - data:/app/data
    env_file:
      - .env
    environment:
      - PYTHONUNBUFFERED=1
      - REDIS_URL=redis://redis:6379/0
      - C_FORCE_ROOT=false  # Disable Celery root warning
      # PyTorch CUDA memory management optimizations
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:128
      - CUDA_LAUNCH_BLOCKING=0
    depends_on:
      - redis
      - backend
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - whisper_network

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: whisper_frontend
    ports:
      - "0.0.0.0:${FRONTEND_PORT:-5173}:5173"  # Listen on all interfaces
    volumes:
      - ./frontend:/app
      - /app/node_modules
    environment:
      - VITE_API_URL=${VITE_API_URL:-http://localhost:8000}
      - VITE_API_PROXY_TARGET=http://backend:8000
    depends_on:
      - backend
    restart: unless-stopped
    networks:
      - whisper_network

volumes:
  uploads:
  outputs:
  models:
  data:
  redis_data:

networks:
  whisper_network:
    driver: bridge
